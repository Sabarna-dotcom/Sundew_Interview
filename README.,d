
# **Insurance Cost Prediction – Machine Learning Project**

This project focuses on predicting the insurance cost for an individual using health, lifestyle and medical history parameters.
The goal is to use data-driven techniques so that the insurance company can estimate an optimal premium while minimizing financial risks.
Customers also benefit through fair pricing based on their actual health and habits.

---

## **Project Overview**

Healthcare costs vary widely depending on lifestyle choices, medical history and general well-being.
To support better decision-making, this project develops a machine learning model that predicts insurance cost using the available dataset.

Several important steps were followed to build a complete pipeline:

* understanding the business problem
* performing full EDA
* engineering meaningful features
* encoding categorical data
* scaling numerical values
* training multiple ML models
* selecting the best performing model
* saving the final pipeline components for real-world prediction

---

## **Work Done in This Project**

### **Data Understanding and Cleaning**

The dataset contained mixed numerical and categorical columns related to lifestyle, health habits and medical history.
Missing values were handled and unnecessary identifiers were removed.
Categorical columns were inspected to understand their unique values.

### **Exploratory Data Analysis**

Univariate and bivariate visualizations were created.
Distributions, correlations and relationships with the target variable were explored.
The project also checked for outliers and imbalance in different features.

### **Feature Engineering**

All categorical columns were transformed using Label Encoding.
Numerical columns were standardized using StandardScaler.
This created a clean and consistent dataset suitable for machine learning models.

### **Model Training**

Multiple models were considered: Linear Regression, RandomForest, Gradient Boosting, Support Vector Regression and others.
However, due to processing limitations and execution time, grid search hyperparameter tuning was not applied to all models.
Two models were trained without GridSearchCV with clear comments explaining why more complex tuning was avoided.

The saved models include:

* Linear Regression
* Support Vector Regression

Performance was measured using R² score, RMSE and MAE.

### **Model Selection**

The model with the highest R² score was automatically selected as the best-performing model and stored for future use.

### **Saving the Pipeline**

To enable seamless real-world prediction, the following were saved using joblib:

* the best model
* the StandardScaler
* all label encoders for categorical columns
* feature order to ensure correct prediction input

This allows us to load everything later and perform prediction with new user input.

---

## **Future Enhancements**

Due to system constraints and time limitations, several modern engineering improvements were not added but can be implemented:

* a FastAPI or Flask REST API service to expose the prediction model
* a full Docker container for deployment
* CI/CD integration using GitHub Actions
* advanced hyperparameter tuning with full GridSearchCV
* SHAP or LIME explainability dashboards
* interactive front-end interface for user-friendly predictions
* cloud deployment on AWS, GCP or Azure

These additions would make the project production-ready and suitable for enterprise environments.

---

## **How to Use the Model**

The notebook saves:

* best_insurance_model.pkl
* scaler.pkl
* label_encoders.pkl
* feature_order.pkl

By loading these files, you can prepare any new user data, pass it through encoders and the scaler and run model.predict() to obtain the estimated insurance cost.

---

## **Conclusion**

This project demonstrates a complete end-to-end machine learning workflow for insurance cost prediction.
It highlights data preprocessing, feature engineering, model evaluation and pipeline saving in a structured and reproducible way.
The work can be extended with modern API frameworks, containerization and cloud deployment if hardware and time permit.

---
